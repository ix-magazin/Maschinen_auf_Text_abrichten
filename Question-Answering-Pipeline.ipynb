{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Beschreibung: Colab-Notebook\n",
    "https://colab.research.google.com/github/Clemes123/exQA-Ausprobieren/Question-Answering-Pipeline.ipynb //funktioniert nur wenn public\n",
    "(sonst zum Testen einfach Colab starten und dort unter \"Upload\" dieses Notebook hochladen)\n",
    "\n",
    "ACHTUNG: In Colab muss unter \"Runtime\" -> \"Change runtime type\" die\n",
    "Hardwarebeschleunigung auf GPU gesetzt werden, sonst dauert die Indizierung\n",
    "der Dokumente mit Dense Passage Retrieval sehr lange."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Haystack aufsetzen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "pip install --upgrade pip\n",
    "pip install farm-haystack[colab,ocr,preprocessing,file-conversion,pdf]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Document Store befüllen"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Dokumente konvertieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from haystack.utils import convert_files_to_docs\n",
    "from haystack.nodes import PreProcessor\n",
    "\n",
    "document_dir = \"./LHM_website_dataset\"\n",
    "all_docs = convert_files_to_docs(dir_path=document_dir)\n",
    "\n",
    "preprocessor = PreProcessor(\n",
    "    split_by=\"word\",\n",
    "    split_length=100,\n",
    "    split_overlap=10,\n",
    "    split_respect_sentence_boundary=True,\n",
    ")\n",
    "\n",
    "docs = preprocessor.process(all_docs)\n",
    "\n",
    "print(f\"n_files_input: {len(all_docs)}\\nn_docs_output: {len(docs)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### zu Document Store hinzufügen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from haystack.document_stores import InMemoryDocumentStore\n",
    "\n",
    "document_store = InMemoryDocumentStore()\n",
    "document_store.write_documents(docs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### mit Dense Passage Retrieval indizieren\n",
    "Hier wird für das `query_embedding_model` und `passage_embedding_model` ein\n",
    "DPR-Modell-Paar eingesetzt, das speziell auf den LHM-Datensatz feinjustiert\n",
    "wurde. Für allgemeine Anwendungsfälle empfehlen wir folgendes Modell-Paar:\n",
    "```\n",
    "query_embedding_model=\"deepset/gbert-base-germandpr-question_encoder\",\n",
    "passage_embedding_model=\"deepset/gbert-base-germandpr-ctx_encoder\",\n",
    "```\n",
    "Dabei ist zu beachten, dass Dense-Passage-Retrieval auf Texten mit\n",
    "dem Modell unbekannten Fachwörtern nicht gut funktioniert. In diesem Fall\n",
    "sollte ein eigenes DPR-Modell feinjustiert werden (siehe Artikel siehe\n",
    "Artikel \"Dense Passage Retrieval für die eigene Domäne\" aus Heft iX 6/2023).\n",
    "\n",
    "Falls das nicht möglich ist, bietet sich alternativ der klassische\n",
    "stichwortbasierte BM25-Algorithmus an. Dafür in der vorangegangenen Zelle\n",
    "`use_bm25` in `document_store = InMemoryDocumentStore(use_bm25=True)` auf\n",
    "`True` setzen und die nachfolgende Zelle nicht ausführen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from haystack.nodes import DensePassageRetriever\n",
    "\n",
    "retriever = DensePassageRetriever(document_store,\n",
    "                                      query_embedding_model=\"schreon/xnext-lhm_queries_encoder\",\n",
    "                                      passage_embedding_model=\"schreon/xnext-lhm_passages_encoder\",\n",
    "                                      embed_title=False)\n",
    "\n",
    "document_store.update_embeddings(retriever)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Reader-Komponente mit QA-Modell initialisieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from haystack.nodes import FARMReader\n",
    "\n",
    "reader = FARMReader(model_name_or_path=\"deepset/gelectra-base-germanquad\", use_gpu=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Pipeline instanziieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from haystack.pipelines import ExtractiveQAPipeline\n",
    "\n",
    "pipeline = ExtractiveQAPipeline(reader, retriever)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Pipeline ausführen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prediction = pipeline.run(\n",
    "    query=\"Was darf ich mit einem Jagdschein?\",\n",
    "    params={\n",
    "        \"Retriever\": {\"top_k\": 10},\n",
    "        \"Reader\": {\"top_k\": 5}\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from haystack.utils import print_answers\n",
    "\n",
    "print_answers(\n",
    "    prediction,\n",
    "    details=\"minimum\"   ## `minimum`, `medium` oder `all`\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
